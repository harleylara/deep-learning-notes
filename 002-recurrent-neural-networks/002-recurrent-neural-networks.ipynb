{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32dd989f-2398-452f-a2ec-ed253ad21e55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "% Bold symbol definition\n",
       "$\n",
       "\\newcommand{\\bm}[1]{{\\boldsymbol #1}}\n",
       "$\n",
       "\n",
       "% Vectors\n",
       "$\n",
       "\\def\\vzero{{\\bm{0}}}\n",
       "\\def\\vone{{\\bm{1}}}\n",
       "\\def\\vmu{{\\bm{\\mu}}}\n",
       "\\def\\vtheta{{\\bm{\\theta}}}\n",
       "\\newcommand{\\v}[1]{{\\bm{#1}}}\n",
       "$\n",
       "\n",
       "% Elements of vectors\n",
       "$\n",
       "\\def\\evalpha{{\\alpha}}\n",
       "\\def\\evbeta{{\\beta}}\n",
       "\\def\\evepsilon{{\\epsilon}}\n",
       "\\def\\evlambda{{\\lambda}}\n",
       "\\def\\evomega{{\\omega}}\n",
       "\\def\\evmu{{\\mu}}\n",
       "\\def\\evpsi{{\\psi}}\n",
       "\\def\\evsigma{{\\sigma}}\n",
       "\\def\\evtheta{{\\theta}}\n",
       "\\def\\eva{{a}}\n",
       "\\def\\evb{{b}}\n",
       "\\def\\evc{{c}}\n",
       "\\def\\evd{{d}}\n",
       "\\def\\eve{{e}}\n",
       "\\def\\evf{{f}}\n",
       "\\def\\evg{{g}}\n",
       "\\def\\evh{{h}}\n",
       "\\def\\evi{{i}}\n",
       "\\def\\evj{{j}}\n",
       "\\def\\evk{{k}}\n",
       "\\def\\evl{{l}}\n",
       "\\def\\evm{{m}}\n",
       "\\def\\evn{{n}}\n",
       "\\def\\evo{{o}}\n",
       "\\def\\evp{{p}}\n",
       "\\def\\evq{{q}}\n",
       "\\def\\evr{{r}}\n",
       "\\def\\evs{{s}}\n",
       "\\def\\evt{{t}}\n",
       "\\def\\evu{{u}}\n",
       "\\def\\evv{{v}}\n",
       "\\def\\evw{{w}}\n",
       "\\def\\evx{{x}}\n",
       "\\def\\evy{{y}}\n",
       "\\def\\evz{{z}}\n",
       "$\n",
       "\n",
       "% Matrix\n",
       "$\n",
       "\\def\\mBeta{{\\bm{\\beta}}}\n",
       "\\def\\mPhi{{\\bm{\\Phi}}}\n",
       "\\def\\mLambda{{\\bm{\\Lambda}}}\n",
       "\\def\\mSigma{{\\bm{\\Sigma}}}\n",
       "\\newcommand{\\m}[1]{{\\bm{#1}}}\n",
       "$"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Latex\n",
    "Latex(filename=\"../notation.tex\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f38cc4-262f-4dfb-ba59-0714f77fa47d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3558e3b-278d-44c2-b451-aff51e7a3862",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068b9022-f255-4778-a5aa-5fc9bfc151b5",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<center><img src=\"./assets/simplified-representation.svg\"></center>\n",
    "\n",
    "<center><img src=\"./assets/bottom-up-representation.svg\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7324e74-7260-43b5-aca4-2285d8b0630b",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Sequence Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc388e28-378c-448b-9f18-6d2b9ca67c52",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<center><img src=\"./assets/sequence-models.svg\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165c428c-ea72-4c91-a2d9-8c8bfd7c5c01",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Feed-Fordward Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7cf911c-d21f-4002-9cf9-6213b5d97b3d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=\"./assets/feed-fordward-networks.svg\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9de6818-74b1-4293-9f91-0762fd22a407",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Handling Individual Time Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368f5283-52e7-425c-b7b4-f7c0fbe23c1f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=\"./assets/individual-time-steps.svg\"></center>\n",
    "\n",
    "<center><img src=\"./assets/relationship-of-individual-time-steps.svg\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f240cd9e-c283-4910-877c-b1327f3ca3ed",
   "metadata": {},
   "source": [
    "## Neurons with Recurrence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9c137f-27a7-4e02-bb66-97b26f9650b3",
   "metadata": {},
   "source": [
    "<center><img src=\"./assets/neurons-with-recurrence.svg\"></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca56acc8-5eb6-44b8-8690-f472d9230d03",
   "metadata": {},
   "source": [
    "## Recurrent Neural Networks (RNNs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22d27e9-d5c4-40bb-9196-48af83ad3302",
   "metadata": {},
   "source": [
    "<center><img src=\"./assets/cell-state.svg\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564a8c3c-f901-4071-aeab-831af59a0e48",
   "metadata": {},
   "source": [
    "**RNNs** have a **state $h_t$** that is update **at each time step** as a sequence is processed\n",
    "\n",
    "$$\n",
    "h_{t} = f_{\\mathbf{W}}(x_t, h_{t-1})\n",
    "$$\n",
    "\n",
    "> Note: The same function and set of parameters are used at every time step\n",
    "\n",
    "\n",
    "**Code Intuition:**\n",
    "\n",
    "```python\n",
    "my_rnn = RNN()\n",
    "hidden_state = [0, 0, 0, 0]\n",
    "\n",
    "sentence = [\"I\", \"am\", \"learning\", \"recurrent\", \"neural\"]\n",
    "\n",
    "for word in sentence:\n",
    "    prediction, hidden_state = my_rnn(word, hidden_state)\n",
    "   \n",
    "next_word_prediction = prediction\n",
    "\n",
    "# >>> \"networks\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0770e148-3609-4df3-b312-85df8fb21ebd",
   "metadata": {},
   "source": [
    "## RNNs - Update Cell State and Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018528f6-9ab8-4242-84a5-0d904e507074",
   "metadata": {},
   "source": [
    "<center><img src=\"./assets/update-cell-state-and-output.svg\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f82e03e-6bc3-441a-8c9b-91a5fc55844c",
   "metadata": {},
   "source": [
    "## Computational Graph Across Time\n",
    "\n",
    "Re-use the **same weights matrices** at every time step\n",
    "\n",
    "<center><img src=\"./assets/computational-graph-across-time.svg\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e15bef1-27fc-479d-a3ff-9ccc62674fa3",
   "metadata": {},
   "source": [
    "### RNNs: Forward pass\n",
    "\n",
    "<center><img src=\"./assets/rnn-loss.svg\"></center>\n",
    "\n",
    "**RNNs Code Intuition**\n",
    "\n",
    "```python\n",
    "class MyRNNCell(tf.keras.layers.Layer):\n",
    "    def __init__(self, rnn_units, input_dim, output_dim):\n",
    "        super(MyRNNCell, self).__init__()\n",
    "        \n",
    "        # Initialize weight matrices\n",
    "        self.W_xh = self.add_weight([rnn_units, input_dim])\n",
    "        self.W_hh = self.add_weight([rnn_units, rnn_units])\n",
    "        self.W_hy = self.add_weight([output_dim, rnn_units])\n",
    "        \n",
    "        # Initialize hidden state to zeros\n",
    "        self.h = tf.zeros([rnn_units, 1])\n",
    "        \n",
    "    def call(self, x):\n",
    "        # Update the hidden state\n",
    "        self.h = tdf.math.tanh(self.W_hh * self.h + self.W_xh * x)\n",
    "        \n",
    "        # Compute the output\n",
    "        output = self.W_hy * self.h\n",
    "        \n",
    "        # Return the current output and hidden state\n",
    "        return output, self.h\n",
    "\n",
    "```\n",
    "\n",
    "**RNNs TensorFlow**\n",
    "\n",
    "```python\n",
    "tf.keras.layers.SimpleRNN(rnn_units)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9433ad-9e3d-422f-abc2-a6a7a897707a",
   "metadata": {},
   "source": [
    "## Sequence Modelling: Design Criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7044957-77f4-4517-8573-2b2457d725ed",
   "metadata": {},
   "source": [
    "To model sequences, we need to:\n",
    "\n",
    "1. Handle **variable-length** sequences\n",
    "2. Track **long-term** dependencies\n",
    "3. Maintain information about **order**\n",
    "4. **Share parameters** across the sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886e4c31-d274-49a8-ae23-e3e6df3b1d55",
   "metadata": {},
   "source": [
    "### Encoding Language for a Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1323aa70-93da-4beb-9f7c-2480539e0532",
   "metadata": {},
   "source": [
    "<center><img src=\"./assets/encoding.svg\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fd4cc4-4a35-4d44-a0dc-ba23b2f17581",
   "metadata": {},
   "source": [
    "### Embedding\n",
    "\n",
    "Let's use the following sentence as an example: **\"This morning I took my dog for a walk\"** \n",
    "\n",
    "<center><img src=\"./assets/embedding.svg\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f765a69-1177-43cf-b324-a85d7e386916",
   "metadata": {},
   "source": [
    "### Handle Variable Sequence Lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b93c41a-1748-478f-af1c-55e4df6161c8",
   "metadata": {},
   "source": [
    "Examples:\n",
    "- *\"The food was great\"*/\n",
    "- *\"We visited a restaurant for launch\"*\n",
    "- *\"We are learning sequence modelling\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604f1f90-822d-4a9a-8ca6-549ccdd5f800",
   "metadata": {},
   "source": [
    "### Model Long-Term Dependencies\n",
    "\n",
    "We need information from **the distant past** to accurately predict the correct word.\n",
    "\n",
    "Example:\n",
    "- *\"Spain is where I grew up, but I now live in Berlin. I speak fluent ____\"*\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee7bbe7-06ba-4a68-991f-09350b7d9a69",
   "metadata": {},
   "source": [
    "### Capture Differences in Sequence Order"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7f5a97-b5cc-4101-82f7-339261a4cf78",
   "metadata": {},
   "source": [
    "We need to be able to capture differences in sequence order which could result in differences in the overall meaning.\n",
    "\n",
    "Example:\n",
    "- *\"The food was good, not bad at all\"* $\\neq$ *\"The food was bad, not good at all\"*\n",
    "\n",
    "In this case where we havo two sentences that have opposite semantic meaning but have the **same words** with the **same counts** just in a **different order**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bd0cc1-2262-486e-a653-b624e685f585",
   "metadata": {},
   "source": [
    "## Backpropagation Through Time (BPTT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd3c5ba-b9de-48f6-96a1-12e4c0d5e15d",
   "metadata": {},
   "source": [
    "<center><img src=\"./assets/bptt.svg\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b0d6d3-b972-4797-9262-8fa613b3a3c7",
   "metadata": {},
   "source": [
    "## Standard RNN Gradient Flow\n",
    "\n",
    "Computing the gradient wrt $h_0$ involve **many factors of $W_{hh}$ + repeated gradient computation**\n",
    "\n",
    "<center><img src=\"./assets/gradient-flow.svg\"></center>\n",
    "\n",
    "<center><img src=\"./assets/problems.svg\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac3b089-d22f-4285-82ac-5340f50447a8",
   "metadata": {},
   "source": [
    "## Problem: Vanishing Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43c2852-b4fb-476e-89e2-03be7ac93b43",
   "metadata": {},
   "source": [
    "**Why are vanishing gradients a problem?**\n",
    "- Multiply many small numbers together\n",
    "- Error due to further back time steps have smaller and smaller gradients\n",
    "- Bias parameters to capture short-term dependencies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeaf1032-c571-4680-89a5-7e63ea834800",
   "metadata": {},
   "source": [
    "## Trick #1: Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3565aa8-d768-42cb-b67c-56b85aa329d2",
   "metadata": {},
   "source": [
    "Whe can smartly select the activation function our networks use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ae9992-f051-4e3e-b453-5d07279e2c68",
   "metadata": {},
   "source": [
    "## Trick #2: Parameter Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ecdeb5-e2de-4753-86c1-9526002ecb7f",
   "metadata": {},
   "source": [
    "- Initialize **weights** to identtity matrix\n",
    "- Initialize **biases** to zero\n",
    "\n",
    "$$\n",
    "I_n = \\begin{bmatrix}\n",
    "1 && 0 && 0 && \\cdots && 0 \\\\\n",
    "0 && 1 && 0 && \\cdots && 0 \\\\\n",
    "0 && 0 && 1 && \\cdots && 0 \\\\\n",
    "\\vdots && \\vdots && \\vdots && \\ddots && \\vdots \\\\\n",
    "0 && 0 && 0 && \\cdots && 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "This helps prevent the weights from shrinking to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1b937f-3b86-49b3-84bb-1ff10502436a",
   "metadata": {},
   "source": [
    "## Trick #3: Gated Cells (most used)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ec33b0-c2e3-44cd-88e4-4a770cd1dc9c",
   "metadata": {},
   "source": [
    "Idea: use a more **complex recurrent unit with gates** to control what information is passed through.\n",
    "\n",
    "\n",
    "<center><img src=\"./assets/gated-cell.svg\"></center>\n",
    "\n",
    "**Long Short Term Memory (LSTM)** networks rely on a gated cell to track information throughout many time steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365b5ed1-ac08-4083-9179-2d150b043071",
   "metadata": {},
   "source": [
    "## Long Short Term Memory (LSTM) Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e542fe-970c-4b74-98cf-fd4d3306d590",
   "metadata": {},
   "source": [
    "In a standard RNN, repeating modules contain a **simple computation node**\n",
    "\n",
    "<center><img src=\"./assets/simple-lstm.svg\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e52bb72-ea69-48e7-b3ec-9202b6c9fff0",
   "metadata": {},
   "source": [
    "### Control Information Flow\n",
    "\n",
    "LSTM modules contain **computational blocks** that **control information flow**\n",
    "\n",
    "<center><img src=\"./assets/control-information-flow.svg\"></center>\n",
    "\n",
    "LSTM cells are able to track information throughout many timesteps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4820ed1c-2952-4198-ae8b-4c42766fb6d9",
   "metadata": {},
   "source": [
    "Information is **added** or **removed** through structures called **gates**\n",
    "\n",
    "<center><img src=\"./assets/add.svg\"></center>\n",
    "\n",
    "Gates optionally let information through, for example via a sigmoid neural net layer and pointwise multiplication."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e196e80-97ef-48a6-95de-20a08d6f5a4c",
   "metadata": {},
   "source": [
    "1. Forget\n",
    "2. Store\n",
    "3. Update\n",
    "4. Ouput\n",
    "\n",
    "The **output gate** controls what information is sent to the next time step\n",
    "\n",
    "<center><img src=\"./assets/lstm-steps.svg\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1070787e-fe6b-4d34-823a-3894da12d995",
   "metadata": {},
   "source": [
    "### Gradient Flow\n",
    "\n",
    "This is done by maintaining the separate cell state $c_t$ across which the actual gradient computation, so taking the derivative of the loss with respect to the weights updating the weights and shifting the weights in response occurs with respect to this separately maintained cell state $c_t$.\n",
    "\n",
    "<center><img src=\"./assets/control-information-flow-gradient.svg\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879a28ec-e0d2-42fc-9271-373d3fc0e26a",
   "metadata": {},
   "source": [
    "## LSTMs: Key Concepts\n",
    "\n",
    "1. Maintain a **separate cell state** from what is outputted\n",
    "2. Use **gates** to control the **flow of information**\n",
    "    - **Forget** gate gets rid of irrelevant information\n",
    "    - **Store** relevant information from current input\n",
    "    - Selectively **update** cell state\n",
    "    - **Output** gate returns a filtered version of the cell state\n",
    "3. Backpropagation through time with **uninterrupted gradientw flow**"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
