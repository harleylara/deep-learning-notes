{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43bfc409-d5f5-4418-9334-e71a3edce30c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\input{commands.tex}\n",
       "\n",
       "% Bold symbol definition\n",
       "$\n",
       "\\newcommand{\\bm}[1]{{\\boldsymbol #1}}\n",
       "$\n",
       "\n",
       "% Vectors\n",
       "$\n",
       "\\def\\vzero{{\\bm{0}}}\n",
       "\\def\\vone{{\\bm{1}}}\n",
       "\\def\\vmu{{\\bm{\\mu}}}\n",
       "\\def\\vtheta{{\\bm{\\theta}}}\n",
       "\\newcommand{\\v}[1]{{\\bm{#1}}}\n",
       "$\n",
       "\n",
       "% Elements of vectors\n",
       "$\n",
       "\\def\\evalpha{{\\alpha}}\n",
       "\\def\\evbeta{{\\beta}}\n",
       "\\def\\evepsilon{{\\epsilon}}\n",
       "\\def\\evlambda{{\\lambda}}\n",
       "\\def\\evomega{{\\omega}}\n",
       "\\def\\evmu{{\\mu}}\n",
       "\\def\\evpsi{{\\psi}}\n",
       "\\def\\evsigma{{\\sigma}}\n",
       "\\def\\evtheta{{\\theta}}\n",
       "\\def\\eva{{a}}\n",
       "\\def\\evb{{b}}\n",
       "\\def\\evc{{c}}\n",
       "\\def\\evd{{d}}\n",
       "\\def\\eve{{e}}\n",
       "\\def\\evf{{f}}\n",
       "\\def\\evg{{g}}\n",
       "\\def\\evh{{h}}\n",
       "\\def\\evi{{i}}\n",
       "\\def\\evj{{j}}\n",
       "\\def\\evk{{k}}\n",
       "\\def\\evl{{l}}\n",
       "\\def\\evm{{m}}\n",
       "\\def\\evn{{n}}\n",
       "\\def\\evo{{o}}\n",
       "\\def\\evp{{p}}\n",
       "\\def\\evq{{q}}\n",
       "\\def\\evr{{r}}\n",
       "\\def\\evs{{s}}\n",
       "\\def\\evt{{t}}\n",
       "\\def\\evu{{u}}\n",
       "\\def\\evv{{v}}\n",
       "\\def\\evw{{w}}\n",
       "\\def\\evx{{x}}\n",
       "\\def\\evy{{y}}\n",
       "\\def\\evz{{z}}\n",
       "$\n",
       "\n",
       "% Matrix\n",
       "$\n",
       "\\def\\mBeta{{\\bm{\\beta}}}\n",
       "\\def\\mPhi{{\\bm{\\Phi}}}\n",
       "\\def\\mLambda{{\\bm{\\Lambda}}}\n",
       "\\def\\mSigma{{\\bm{\\Sigma}}}\n",
       "\\newcommand{\\m}[1]{{\\bm{#1}}}\n",
       "$"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Latex\n",
    "Latex(filename=\"../notation/notation.tex\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25da1f18-2152-440a-842a-175753d8fe4b",
   "metadata": {},
   "source": [
    "# Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e924c0a0-9ab8-449f-a989-608315113cfe",
   "metadata": {},
   "source": [
    "**Supervised Learning**\n",
    "\n",
    "- Data: $(x, y)$\n",
    "- Goal: Learn function to map $x->y$\n",
    "\n",
    "\n",
    "**Unsupervised Learning**\n",
    " \n",
    "- Data: $x$ is data, NO labels!\n",
    "- Goal: Learn underlying\n",
    "\n",
    "**Reinforcement Learning**\n",
    "\n",
    "- Data: State-action pairs\n",
    "- Goal: Maximize future rewards over many time steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6cb128-d05c-47e6-b0d5-b1e367b46173",
   "metadata": {},
   "source": [
    "## Key Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71aa245-d30f-4a71-bb22-159b017eb1a8",
   "metadata": {},
   "source": [
    "<center><img src=\"./assets/concepts.svg\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76480146-4a2d-4cfc-8179-388dbeec3d48",
   "metadata": {},
   "source": [
    "- **Agent:** Takes actions\n",
    "- **Environment:** The world in which the agent exist and operates\n",
    "- **Actions $a_t$:** A move the agent can make in the environment\n",
    "- **Action space $A$:** The set of possible actions an agent can make in the environment\n",
    "- **Observations:** Of the environment after taking actions.\n",
    "- **State $S_{t+1}$ :** a situation which the agent perceives\n",
    "- **Reward $r_t$:** Feedback the measures the success or fail of the agent's actions\n",
    "\n",
    "\n",
    "Total reward (Return):\n",
    "$$\n",
    "R_t = \\sum_{i=t}^{\\infty} \\gamma^i r_i\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\gamma: discount factor; 0 < \\gamma < 1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63c9365-90c1-4c88-ab32-ab71ae4a8bd2",
   "metadata": {},
   "source": [
    "## Defining the Q-function\n",
    "\n",
    "$$\n",
    "R_t = r_t + \\gamma r_{t+1} + \\gamma^{2} r_{t+2} + \\cdots\n",
    "$$\n",
    "\n",
    "Total reward $R_t$ is the discounted sum of all rewards obtained from time $t$\n",
    "\n",
    "$$\n",
    "Q(s_t, a_t) = \\mathbb{E} [R_t | s_t, a_t]\n",
    "$$\n",
    "\n",
    "The Q-Function captures the **expected total future reward** an agent in **state $s$** can receive by executing a certain **action $a$**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a53b7e-51d7-401e-90fd-3283f6f6f2f9",
   "metadata": {},
   "source": [
    "## How to take actions given a Q-fuction\n",
    "\n",
    "Ultimately the agent needs a **policy $\\pi(s)$** to infer **the best action to take** at its state $s$\n",
    "\n",
    "**Strategy:** The policy should choose an action that maximizes future reward\n",
    "\n",
    "$$\n",
    "\\pi(s) = \\underset{a}{argmax} Q(s, a)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04056c87-c8ac-447a-ac79-fc76ccc696ee",
   "metadata": {},
   "source": [
    "## Deep Reinforcement Learning Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31da080-3093-4150-b6c3-48d921d36ef5",
   "metadata": {},
   "source": [
    "<center><img src=\"./assets/algorithms.svg\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ac13e8-4e4f-4f33-94d4-a2968f10a047",
   "metadata": {},
   "source": [
    "**Value Learning**\n",
    "- Find $Q(s, a)$\n",
    "- $a = \\underset{a}{argmax} Q(s, a)$\n",
    "\n",
    "**Policy Learning**\n",
    "- Find $\\pi(s)$\n",
    "- Sample $a \\sim \\pi(s)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df65b01-ef4e-42fc-b4ed-1b5f65a27c17",
   "metadata": {},
   "source": [
    "## Q-function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260cfb80-a52a-4c03-b035-593931753662",
   "metadata": {},
   "source": [
    "<center><img src=\"./assets/q-function.svg\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294d791d-9df2-4c5b-b8da-0e410b5766c8",
   "metadata": {},
   "source": [
    "## Q-Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bca5117-bdd2-43e8-92b1-af1023efea3a",
   "metadata": {},
   "source": [
    "<center><img src=\"./assets/q-loss.svg\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4a232d-fc7c-4ed3-81a7-8e0d21095599",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee347373-b0e7-4308-9ea5-c394a8bc5e39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2427d5-a0e7-470a-a8ff-ffe33f93c873",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4d0851-3b36-4164-bbbf-88b2a30275c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "13cfb713-1226-4bc5-9bc3-49ceaa0b1f85",
   "metadata": {},
   "source": [
    "## Downsides of Q-learning\n",
    "\n",
    "**Complexity:**\n",
    "- Can model scenarios where the action space is discrete and small\n",
    "- Can not handle continuos action spaces\n",
    "\n",
    "**Flexibility:**\n",
    "- Policy is deterministically computed from the Q function by maximazing the reward -> can not learn stochastic policies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e028f9d3-476b-4fbe-9ce9-0a0c102f185e",
   "metadata": {},
   "source": [
    "## Policy Gradient (PG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e912f0-a5b0-4d63-8834-6d21032c62db",
   "metadata": {},
   "source": [
    "**Policy Gradient**: Directly optimize the policy $\\pi (s)$\n",
    "\n",
    "<center><img src=\"./assets/pq.svg\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900ff923-16af-4f22-b857-bf2057e591c0",
   "metadata": {},
   "source": [
    "## Discrete vs Continuous Action Spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17abc3b2-40d7-4189-91bf-d0e23fda87c8",
   "metadata": {},
   "source": [
    "- **Discrete action space:** left, stay, right\n",
    "\n",
    "<center><img src=\"./assets/discrete.svg\"></center>\n",
    "\n",
    "- **Continuous action space:** how fast should I move?\n",
    "\n",
    "<center><img src=\"./assets/continuous.svg\"></center>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
